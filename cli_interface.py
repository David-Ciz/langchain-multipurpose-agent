import logging
import os

import click
from dotenv import load_dotenv

from documentation_loader import update_docs_database
from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler
from langchain import hub
from langchain.agents import initialize_agent, create_react_agent, AgentExecutor
from langchain.agents.agent_types import AgentType
from langchain_community.tools.ddg_search import DuckDuckGoSearchRun
from langchain_core.tools import Tool
from langchain_experimental.agents.agent_toolkits import create_csv_agent
from langchain_openai import ChatOpenAI, OpenAI
from dotenv import load_dotenv
load_dotenv()
logger = logging.getLogger(__name__)
INDEX_NAME = os.environ["INDEX_NAME"]


@click.group()
def cli():
    pass


@cli.command("update-docs")
@click.option("--documentation_url",type=str, default="https://ibm.github.io/ibm-generative-ai/",
              help="Optional argument if the url changes. defaults to https://ibm.github.io/ibm-generative-ai/")
def update_docs_database_cli(documentation_url: str):
    """
    Updates the PINECONE database with new documents when called. It expects the document structure to be
    generated by Sphinx.
    """
    update_docs_database()


@cli.command("start-chat")
@click.option("--verbose", type=bool, default="False", help="Verbosity of chain of though")
def start_chat_cli(verbose: bool):
    """
    Starts an interactive chat with the agent in the terminal.
    """

    csv_agent = create_csv_agent(
        OpenAI(temperature=0),
        "titanic_modified.csv",
        verbose=verbose,
        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    )

    web_search = DuckDuckGoSearchRun(name="Search")

    tools = [web_search,
             Tool(
                 name="titanic csv information",
                 func=csv_agent.run,
                 description=f"""
                Useful for when you need to answer questions about titanic data stored in pandas dataframe 'df'. 
                Runs python pandas operations on 'df' to help you get the right answer.         
                """
             ),
             ]
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, streaming=True)

    prompt = hub.pull("hwchase17/react")

    agent = create_react_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=verbose, handle_parsing_errors=True, callbacks=[FinalStreamingStdOutCallbackHandler()])

    # TODO: explore this:
    # /home/david/anaconda3/envs/langchain/lib/python3.12/site-packages/langchain/agents/mrkl/base.py

    # agent.run("how many rows are there?")
    while True:
        user_input = input()
        if user_input == "exit":
            break
        else:
            if verbose:
                agent_executor.invoke({"input": f"{user_input}"})
            else:
                result = agent_executor.invoke({"input": f"{user_input}"})
                print(result["output"])

# TODO: can't make it in time, the cli would need to call methods that are not bound to streamlit but fairly easy to add. Same for the api, just add fastapi decoration to the methods @app.post("/prompt")
# @cli.command()
# def call_documentation_qa_agent(prompt):
#     memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, chat_memory=msgs,
#                                       output_key=memory_output_key)
#     executor = initiate_documentation_agent(INDEX_NAME, chat_box, memory)
#     get_documentation_qa_response(executor, chat_box, prompt)
#
# cli.command()
# def call_tools_agent(prompt):
#     memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, chat_memory=msgs,
#                                       output_key=memory_output_key)
#     executor = initiate_documentation_agent(INDEX_NAME, chat_box, memory)
#     get_documentation_qa_response(executor, chat_box, prompt)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    cli()